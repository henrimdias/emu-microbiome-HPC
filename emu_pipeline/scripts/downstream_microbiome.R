#!/usr/bin/env Rscript
# downstream_microbiome.R
#
# Article:
# Dias, H.M., et al. Reproducible Emu-based workflow for high-fidelity soil and
# plant microbiome profiling on HPC clusters. Bio-protocol. 2025.
#
# Author (script):
# Henrique M. Dias
#
# Affiliation:
# South Dakota State University
#
# Date:
# 2025
#
# Description:
# Downstream analyses for full-length 16S microbiome data generated by Emu
# within the workflow described in the article above. The script performs:
#   - Rarefaction curves (library size vs. expected richness)
#   - Taxonomic composition barplots at a chosen rank
#   - Alpha diversity (S_obs, Chao1, Shannon, Simpson, Pielou) with
#     global tests (ANOVA / Kruskal–Wallis) and post-hoc comparisons
#   - Beta diversity (Bray, Jaccard, Aitchison/CLR), PCoA, NMDS,
#     PERMANOVA and betadisper
#   - Export of key diagnostics to CSV and publication-quality PNG / SVG plots
#
# The code assumes that:
#   - A metadata table is available with one row per sample
#   - A raw count table (taxa x samples) is available
#   - Optionally, a relative abundance table is available; if not, it is computed
#
# Inputs (user configuration below):
#   metadata_path   : path to metadata file (samples in rows)
#   raw_counts_path : path to raw counts table (features in rows, samples in columns)
#   rel_abund_path  : path to relative abundance table (optional; set to NA to compute)
#
# Outputs:
#   - Rarefaction curves, composition, alpha and beta diversity plots (PNG + SVG)
#   - CSV files with alpha statistics, distance matrices, ordination scores,
#     PERMANOVA and betadisper summaries
#
# This script is designed to be called interactively (Rstudio) or via Rscript.
# For full reproducibility, the package versions used in the protocol are listed
# in the manuscript / accompanying documentation.

# ============================================================
# 0) Packages
# ============================================================

pkgs <- c(
  "data.table", "dplyr", "tidyr", "ggplot2", "forcats", "vegan", "stringr",
  "ggrepel", "ggpubr", "rstatix", "broom", "tools", "svglite", "ape"
)

missing_pkgs <- pkgs[!pkgs %in% installed.packages()[, "Package"]]
if (length(missing_pkgs) > 0) {
  stop(
    "The following packages are required but not installed: ",
    paste(missing_pkgs, collapse = ", "),
    ". Please install them before running this script."
  )
}

invisible(lapply(pkgs, library, character.only = TRUE))

# ============================================================
# 1) User configuration (edit for a given dataset)
# ============================================================

# Path to metadata table (one row per sample)
metadata_path   <- "metadata.tsv"

# Path to raw count table (features in rows, samples in columns)
raw_counts_path <- "raw_counts.tsv"

# Path to relative abundance table
rel_abund_path  <- "rela_counts.tsv"

# Column name in metadata containing sample IDs
sample_id_col   <- "SampleID"

# Column name used as the main grouping factor (for colors / statistics)
group_col       <- "Group"

# Target taxonomic rank for composition plots (case-insensitive).
# Typical choices: "kingdom", "phylum", "class", "order", "family", "genus", "species".
tax_level       <- "genus"

# Number of most abundant taxa to display explicitly in composition plots
topN_taxa       <- 16

# Number of interpolation steps for rarefaction curves
raref_steps     <- 100

# Output directory for all plots and tables
EXPORT_DIR <- "downstream_microbiome_output"
dir.create(EXPORT_DIR, showWarnings = FALSE, recursive = TRUE)

# Optional: preprocessing for alpha diversity (filters / rarefaction on raw counts)
filter_for_alpha <- FALSE   # if TRUE, apply filters below (must be used for Chaos1)
min_lib_alpha    <- 1000    # minimum reads per sample for alpha analyses
min_prevalence   <- 2       # minimum number of samples in which a feature must be present
min_total_count  <- 10      # minimum total counts per feature (across all samples)
rarefy_for_alpha <- FALSE   # if TRUE, perform rarefaction for alpha analyses
rare_depth_alpha <- NA      # if NA, uses minimum library size after filtering

# Optional: minimum library size for inclusion in beta diversity
min_lib_beta     <- 1       # 1 removes only zero-library samples

# ============================================================
# 2) Helper functions
# ============================================================

# Robust reader for csv/tsv/txt with automatic separator detection
smart_read <- function(path) {
  data.table::fread(
    path,
    sep         = "auto",
    header      = TRUE,
    data.table  = FALSE,
    check.names = FALSE
  )
}

# Save a ggplot to PNG and/or SVG with explicit sizes per call
save_plot_multi <- function(plot, path_noext,
                            png = TRUE, svg = TRUE,
                            png_width, png_height,
                            png_units = c("in", "cm", "mm"),
                            png_dpi = 300,
                            svg_width, svg_height,
                            svg_units = c("cm", "in", "mm")) {
  dir.create(dirname(path_noext), showWarnings = FALSE, recursive = TRUE)
  
  if (isTRUE(png)) {
    if (missing(png_width) || missing(png_height)) {
      stop("For PNG, provide 'png_width' and 'png_height'.")
    }
    png_units <- match.arg(png_units)
    ggplot2::ggsave(
      filename = paste0(path_noext, ".png"),
      plot     = plot,
      width    = png_width,
      height   = png_height,
      units    = png_units,
      dpi      = png_dpi
    )
  }
  
  if (isTRUE(svg)) {
    if (missing(svg_width) || missing(svg_height)) {
      stop("For SVG, provide 'svg_width' and 'svg_height'.")
    }
    svg_units <- match.arg(svg_units)
    ggplot2::ggsave(
      filename = paste0(path_noext, ".svg"),
      plot     = plot,
      device   = svglite::svglite,
      width    = svg_width,
      height   = svg_height,
      units    = svg_units
    )
  }
}

# Detect taxonomic columns (case-insensitive; treat superkingdom/domain as kingdom)
get_tax_cols <- function(df) {
  ranks_all <- c(
    "kingdom", "phylum", "class", "order", "family",
    "genus", "species", "superkingdom", "domain"
  )
  ln <- tolower(colnames(df))
  keep_idx <- which(ln %in% ranks_all)
  if (!length(keep_idx)) return(character(0))
  cols <- colnames(df)[keep_idx]
  canon_order <- c("kingdom", "phylum", "class", "order", "family", "genus", "species")
  ln_keep <- tolower(cols)
  canon_key <- ifelse(ln_keep %in% c("superkingdom", "domain"), "kingdom", ln_keep)
  cols[order(match(canon_key, canon_order), na.last = TRUE)]
}

# Resolve desired taxonomic level: try exact; otherwise move up/down the hierarchy
resolve_tax_level <- function(df, level) {
  ranks <- c("kingdom", "phylum", "class", "order", "family", "genus", "species")
  cn_raw <- colnames(df)
  cn     <- tolower(trimws(cn_raw))
  level  <- tolower(trimws(level))
  if (level %in% c("superkingdom", "domain")) level <- "kingdom"
  
  hit <- which(cn == level)
  if (length(hit)) return(cn_raw[hit[1]])
  
  pos <- match(level, ranks)
  if (!is.na(pos)) {
    # search upwards
    for (i in seq(pos - 1, 1, by = -1)) {
      hit <- which(cn == ranks[i])
      if (length(hit)) return(cn_raw[hit[1]])
    }
    # search downwards
    for (i in seq(pos + 1, length(ranks), by = 1)) {
      hit <- which(cn == ranks[i])
      if (length(hit)) return(cn_raw[hit[1]])
    }
  }
  tc <- get_tax_cols(df)
  if (length(tc)) return(tc[length(tc)])
  NA_character_
}

# Chao1 richness estimator (simple implementation; with f2 = 0 correction)
chao1_vec <- function(x) {
  x <- as.numeric(x)
  Sobs <- sum(x > 0, na.rm = TRUE)
  f1 <- sum(x == 1, na.rm = TRUE)
  f2 <- sum(x == 2, na.rm = TRUE)
  if (f2 == 0) {
    return(Sobs + ifelse(f1 > 0, (f1 * (f1 - 1)) / (2 * (f2 + 1)), 0))
  }
  Sobs + (f1^2) / (2 * f2)
}

# Rarefaction curves (integer counts; skips zero-library samples)
compute_rarefaction_df <- function(counts_mat, steps = 60) {
  if (nrow(counts_mat) == 0) {
    return(data.frame(SampleID = character(), Reads = numeric(), Richness = numeric()))
  }
  out <- lapply(rownames(counts_mat), function(s) {
    v <- as.numeric(counts_mat[s, ])
    v[!is.finite(v) | v < 0] <- 0
    v <- round(v)
    n <- sum(v, na.rm = TRUE)
    if (!is.finite(n) || n <= 0) {
      return(data.frame(SampleID = s, Reads = 0, Richness = 0))
    }
    m_seq <- unique(round(seq(1, n, length.out = steps)))
    m_seq <- m_seq[m_seq >= 1 & m_seq <= n]
    if (length(m_seq) == 0) m_seq <- n
    rich <- suppressWarnings(sapply(m_seq, function(m) vegan::rarefy(v, sample = m)))
    data.frame(SampleID = s, Reads = m_seq, Richness = as.numeric(rich))
  })
  do.call(rbind, out)
}

# Percentage of variance explained by the first two eigenvalues
percent_from_eig <- function(eig) {
  p <- eig / sum(abs(eig))
  round(100 * p[1:2], 1)
}

# Centered log-ratio transformation (CLR) for compositional data
.clr <- function(x, pseudo = 1e-6) {
  x <- sweep(x, 1, pmax(rowSums(x), 1e-12), "/")
  x[x <= 0] <- pseudo
  l <- log(x)
  sweep(l, 1, rowMeans(l), "-")
}

# Pairwise PERMANOVA comparisons between all group combinations
pairwise_permanova <- function(dist_obj, grouping, permutations = 999) {
  gr  <- factor(grouping)
  lev <- levels(gr)
  res <- data.frame(
    group1 = character(),
    group2 = character(),
    F      = numeric(),
    R2     = numeric(),
    p      = numeric()
  )
  if (length(lev) < 2) return(res)
  for (i in seq_len(length(lev) - 1)) {
    for (j in seq(i + 1, length(lev))) {
      idx <- gr %in% c(lev[i], lev[j])
      sub_meta <- data.frame(g = droplevels(gr[idx]))
      ad <- vegan::adonis2(
        as.dist(as.matrix(dist_obj)[idx, idx]) ~ g,
        data        = sub_meta,
        permutations = permutations,
        by          = "terms"
      )
      res <- rbind(
        res,
        data.frame(
          group1 = lev[i],
          group2 = lev[j],
          F      = ad$F[1],
          R2     = ad$R2[1],
          p      = ad$`Pr(>F)`[1]
        )
      )
    }
  }
  res$p_adj <- p.adjust(res$p, method = "holm")
  res[order(res$p_adj), ]
}

# ============================================================
# 3) Read and prepare input data
# ============================================================

meta <- smart_read(metadata_path) %>% as.data.frame()
stopifnot(sample_id_col %in% colnames(meta))
meta[[sample_id_col]] <- as.character(meta[[sample_id_col]])

if (!group_col %in% colnames(meta)) {
  meta[[group_col]] <- "All"
  message("`", group_col, "` not found in metadata; using a single group 'All'.")
}
sample_ids <- unique(meta[[sample_id_col]])

counts_raw <- smart_read(raw_counts_path) %>% as.data.frame()

# Identify sample columns by intersecting with metadata sample IDs
sample_cols <- intersect(colnames(counts_raw), sample_ids)
if (length(sample_cols) < 2) {
  stop(
    "No sample columns in raw counts matched the metadata column `",
    sample_id_col, "`."
  )
}

# Identify a feature ID column if present; otherwise create one
feature_col <- setdiff(colnames(counts_raw), sample_cols)
feature_col <- if (length(feature_col)) feature_col[1] else "FeatureID"
if (!feature_col %in% colnames(counts_raw)) {
  counts_raw[[feature_col]] <- paste0("feat_", seq_len(nrow(counts_raw)))
}

# Build counts matrix with samples in rows
counts_mat <- counts_raw[, sample_cols, drop = FALSE] |>
  as.matrix() |>
  t()
mode(counts_mat) <- "numeric"
rownames(counts_mat) <- sample_cols
counts_mat[!is.finite(counts_mat)] <- NA
counts_mat[is.na(counts_mat)]      <- 0
counts_mat[counts_mat < 0]         <- 0

# Align counts and metadata by sample ID
common_ids <- intersect(rownames(counts_mat), sample_ids)
counts_mat  <- counts_mat[common_ids, , drop = FALSE]
meta        <- meta[match(common_ids, meta[[sample_id_col]]), , drop = FALSE]
stopifnot(all(rownames(counts_mat) == meta[[sample_id_col]]))

# Relative abundance table (read from file or computed from raw counts)
if (!is.na(rel_abund_path) && file.exists(rel_abund_path)) {
  rel_abund_df <- smart_read(rel_abund_path) %>% as.data.frame()
} else {
  rel_abund_df <- counts_raw
  rel_abund_df[, sample_cols] <- apply(
    rel_abund_df[, sample_cols, drop = FALSE], 2,
    function(x) {
      s <- sum(x, na.rm = TRUE)
      if (s == 0) return(rep(0, length(x)))
      x / s
    }
  )
}

# ============================================================
# 4) Rarefaction curves (raw counts)
# ============================================================

lib_sizes <- rowSums(counts_mat)
counts_mat_rare <- counts_mat[lib_sizes > 0, , drop = FALSE]

rare_df <- compute_rarefaction_df(counts_mat_rare, steps = raref_steps) %>%
  dplyr::left_join(meta, by = setNames(sample_id_col, "SampleID"))

p_rare <- ggplot(
  rare_df,
  aes(x = Reads, y = Richness, color = .data[[group_col]], group = SampleID)
) +
  geom_line(alpha = 0.9, linewidth = 0.7) +
  labs(
    x     = "Subsampled reads",
    y     = "Expected richness (rarefaction)",
    color = group_col,
    title = "Rarefaction curves (vegan::rarefy)"
  ) +
  theme_bw()

print(p_rare)

save_plot_multi(
  p_rare, file.path(EXPORT_DIR, "rarefaction_curves"),
  png_width  = 8,  png_height  = 5,  png_units  = "in",
  svg_width  = 18, svg_height  = 10, svg_units  = "cm"
)

# ============================================================
# 5) Taxonomic composition (aggregate + renormalize per sample)
# ============================================================

sample_cols_rel <- intersect(colnames(rel_abund_df), sample_ids)
if (length(sample_cols_rel) < 1) {
  stop("No sample columns in relative abundance matched the metadata IDs.")
}

tax_cols_rel        <- get_tax_cols(rel_abund_df)
tax_level_resolved  <- resolve_tax_level(rel_abund_df, tax_level)
if (is.na(tax_level_resolved)) {
  if (!feature_col %in% colnames(rel_abund_df)) {
    rel_abund_df[[feature_col]] <- counts_raw[[feature_col]]
  }
  tax_level_resolved <- feature_col
  message("No taxonomy columns found; using `", feature_col, "` as label.")
}

needed_cols <- unique(c(tax_cols_rel, sample_cols_rel))
rel_long <- rel_abund_df %>%
  dplyr::select(dplyr::all_of(needed_cols)) %>%
  tidyr::pivot_longer(
    cols      = dplyr::all_of(sample_cols_rel),
    names_to  = "SampleID",
    values_to = "Abundance"
  ) %>%
  dplyr::mutate(SampleID = as.character(SampleID)) %>%
  dplyr::left_join(meta, by = setNames(sample_id_col, "SampleID"))

# Replace missing taxa labels with "Unassigned"
rel_long[[tax_level_resolved]] <- ifelse(
  is.na(rel_long[[tax_level_resolved]]) |
    rel_long[[tax_level_resolved]] == "",
  "Unassigned",
  rel_long[[tax_level_resolved]]
)

# Aggregate at chosen taxonomic level and renormalize per sample
rel_level <- rel_long %>%
  dplyr::group_by(SampleID, .data[[tax_level_resolved]]) %>%
  dplyr::summarise(
    Abundance = sum(Abundance, na.rm = TRUE),
    .groups   = "drop_last"
  ) %>%
  dplyr::mutate(
    Abundance = Abundance / pmax(sum(Abundance, na.rm = TRUE), 1e-12)
  ) %>%
  dplyr::ungroup()

# Keep top N taxa and collapse remaining taxa as "Other"
top_taxa <- rel_level %>%
  dplyr::group_by(.data[[tax_level_resolved]]) %>%
  dplyr::summarise(
    MeanAbund = mean(Abundance, na.rm = TRUE),
    .groups   = "drop"
  ) %>%
  dplyr::arrange(dplyr::desc(MeanAbund)) %>%
  dplyr::slice_head(n = topN_taxa) %>%
  dplyr::pull(.data[[tax_level_resolved]])

rel_level$TaxonPlot <- ifelse(
  rel_level[[tax_level_resolved]] %in% top_taxa,
  rel_level[[tax_level_resolved]],
  "Other"
)

fill_lab <- gsub("_", " ", tools::toTitleCase(tolower(tax_level_resolved)))

p_comp <- rel_level %>%
  dplyr::left_join(meta, by = "SampleID") %>%
  dplyr::mutate(SampleID = factor(SampleID, levels = unique(SampleID))) %>%
  ggplot(aes(x = SampleID, y = Abundance, fill = TaxonPlot)) +
  geom_col(width = 0.9) +
  labs(
    x    = "Sample",
    y    = "Relative abundance",
    fill = fill_lab,
    title = paste("Taxonomic composition by", fill_lab)
  ) +
  theme_bw() +
  theme(
    axis.text.x =
      element_text(angle = 90, hjust = 1, vjust = 0.5)
  )

print(p_comp)

save_plot_multi(
  p_comp, file.path(EXPORT_DIR, paste0("composition_", tolower(tax_level))),
  png_width  = 10, png_height  = 5,  png_units  = "in",
  svg_width  = 22, svg_height  = 12, svg_units  = "cm"
)

# ============================================================
# 6) Alpha diversity (Chao1 on raw counts; others on relative)
# ============================================================

counts_alpha <- counts_mat
meta_alpha   <- meta

if (filter_for_alpha) {
  # Filter samples by minimum library size
  lib_sizes_alpha <- rowSums(counts_alpha, na.rm = TRUE)
  keep_samples    <- lib_sizes_alpha >= min_lib_alpha
  if (any(!keep_samples)) {
    message(
      "Alpha: removing samples with library size < ", min_lib_alpha, ": ",
      paste(rownames(counts_alpha)[!keep_samples], collapse = ", ")
    )
  }
  counts_alpha <- counts_alpha[keep_samples, , drop = FALSE]
  meta_alpha   <- meta_alpha[
    meta_alpha[[sample_id_col]] %in% rownames(counts_alpha),
    , drop = FALSE
  ]
  
  # Filter features by prevalence and total counts
  prev <- colSums(counts_alpha > 0, na.rm = TRUE)
  tot  <- colSums(counts_alpha,     na.rm = TRUE)
  keep_features <- (prev >= min_prevalence) & (tot >= min_total_count)
  if (any(!keep_features)) {
    message(
      "Alpha: removing ", sum(!keep_features),
      " features with low prevalence / abundance."
    )
  }
  counts_alpha <- counts_alpha[, keep_features, drop = FALSE]
}

if (rarefy_for_alpha) {
  if (nrow(counts_alpha) == 0) {
    stop("Alpha: no samples available after filtering.")
  }
  lib_sizes_alpha <- rowSums(counts_alpha, na.rm = TRUE)
  if (any(lib_sizes_alpha <= 0)) {
    stop("Alpha: zero-library samples present after filtering.")
  }
  if (is.na(rare_depth_alpha)) rare_depth_alpha <- min(lib_sizes_alpha)
  set.seed(123)
  counts_alpha <- vegan::rrarefy(counts_alpha, sample = rare_depth_alpha)
  message("Alpha: rarefied all samples to ", rare_depth_alpha, " reads.")
}

# Compute alpha metrics
S_obs   <- rowSums(counts_alpha > 0, na.rm = TRUE)
Chao1   <- apply(counts_alpha, 1, chao1_vec)
rel_mat_alpha <- counts_alpha / pmax(rowSums(counts_alpha), 1)
Shannon <- vegan::diversity(rel_mat_alpha, index = "shannon")
Simpson <- vegan::diversity(rel_mat_alpha, index = "simpson")
Pielou  <- ifelse(S_obs > 1, Shannon / log(S_obs), NA_real_)

alpha_df <- data.frame(
  SampleID = rownames(counts_alpha),
  S_obs    = S_obs,
  Chao1    = Chao1,
  Shannon  = Shannon,
  Simpson  = Simpson,
  Pielou   = Pielou,
  check.names = FALSE
) %>%
  dplyr::left_join(meta_alpha, by = "SampleID")

alpha_df[[group_col]] <- as.factor(alpha_df[[group_col]])

# Global tests + post-hoc comparisons for alpha metrics
.alpha_stats_for_plot <- function(df, metric, group_col) {
  dd <- df[, c(metric, group_col)]
  names(dd) <- c("value", "group")
  dd <- dd %>%
    dplyr::filter(is.finite(value)) %>%
    tidyr::drop_na()
  
  # Normality by group (Shapiro)
  sh <- dd %>%
    dplyr::group_by(group) %>%
    dplyr::summarise(
      n = dplyr::n(),
      p = ifelse(n >= 3, shapiro.test(value)$p.value, NA_real_),
      .groups = "drop"
    )
  normal_ok <- all(is.na(sh$p) | sh$p > 0.05)
  
  # Homoscedasticity (Levene)
  lev <- tryCatch(
    rstatix::levene_test(value ~ group, data = dd, center = "median"),
    error = function(e) NULL
  )
  homosked_ok <- if (!is.null(lev)) lev$p > 0.05 else FALSE
  
  # Global test + post-hoc
  if (normal_ok && homosked_ok) {
    method <- "ANOVA"
    global <- broom::tidy(aov(value ~ group, data = dd)) %>%
      dplyr::slice(1) %>%
      dplyr::transmute(p = p.value)
    post   <- rstatix::tukey_hsd(dd, value ~ group) %>%
      dplyr::mutate(test = "Tukey")
  } else {
    method <- "Kruskal"
    global <- rstatix::kruskal_test(dd, value ~ group) %>%
      dplyr::transmute(p = p)
    post   <- rstatix::dunn_test(dd, value ~ group, p.adjust.method = "holm") %>%
      dplyr::mutate(test = "Dunn")
  }
  list(
    method    = method,
    global_p  = global$p[1],
    posthoc   = post,
    shapiro   = sh,
    levene    = lev
  )
}

.alpha_plot_annotated <- function(df, metric, ylab, title) {
  st <- .alpha_stats_for_plot(df, metric, group_col)
  y_max <- suppressWarnings(max(df[[metric]], na.rm = TRUE))
  if (!is.finite(y_max)) y_max <- 1
  
  g <- ggplot(
    df, aes(x = .data[[group_col]], y = .data[[metric]],
            fill = .data[[group_col]])
  ) +
    geom_boxplot(alpha = 0.8, outlier.shape = NA) +
    geom_jitter(width = 0.15, alpha = 0.7) +
    labs(
      x     = group_col,
      y     = ylab,
      title = paste0(title, " (", st$method, ")")
    ) +
    theme_bw() +
    theme(legend.position = "none") +
    ggpubr::stat_compare_means(
      method  = ifelse(st$method == "ANOVA", "anova", "kruskal.test"),
      label.y = y_max * 1.05
    )
  
  post <- st$posthoc
  if (!is.null(post) && nrow(post) > 0) {
    if (!"p.adj" %in% names(post)) post$p.adj <- post$p
    if (!"p.adj.signif" %in% names(post)) {
      post$p.adj.signif <- cut(
        post$p.adj,
        breaks = c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),
        labels = c("***", "**", "*", ".", "ns")
      )
    }
    sig <- post %>%
      dplyr::filter(!is.na(p.adj), p.adj <= 0.05) %>%
      dplyr::select(group1, group2, p.adj, p.adj.signif)
    if (nrow(sig) > 0) {
      sig$y.position <- seq(
        from = y_max * 1.12,
        by   = y_max * 0.08,
        length.out = nrow(sig)
      )
      g <- g + ggpubr::stat_pvalue_manual(
        sig,
        label      = "p.adj.signif",
        xmin       = "group1",
        xmax       = "group2",
        y.position = "y.position",
        tip.length = 0.01,
        hide.ns    = TRUE
      )
    }
  }
  g
}

p_alpha1 <- .alpha_plot_annotated(
  alpha_df, "Chao1",
  "Alpha index value",
  "Chao1 richness estimator"
)
p_alpha2 <- .alpha_plot_annotated(
  alpha_df, "Shannon",
  "Alpha index value",
  "Shannon diversity index"
)
p_alpha3 <- .alpha_plot_annotated(
  alpha_df, "Simpson",
  "Alpha index value",
  "Simpson's diversity index"
)
p_alpha4 <- .alpha_plot_annotated(
  alpha_df, "Pielou",
  "Alpha index value",
  "Pielou's evenness index"
)

print(p_alpha1); print(p_alpha2); print(p_alpha3); print(p_alpha4)

# Save alpha diagnostics and plots
save_alpha_diagnostics <- function(df, metric, group_col, outdir = EXPORT_DIR) {
  dir.create(outdir, showWarnings = FALSE, recursive = TRUE)
  dd <- df[, c(metric, group_col)]
  names(dd) <- c("value", "group")
  
  if (length(unique(stats::na.omit(dd$group))) < 2) {
    overall <- data.frame(
      metric  = metric,
      method  = NA,
      p_global = NA,
      note    = "< 2 groups"
    )
    utils::write.csv(
      overall,
      file      = file.path(outdir, paste0("alpha_overall_", metric, ".csv")),
      row.names = FALSE
    )
    return(invisible(NULL))
  }
  
  st <- .alpha_stats_for_plot(df, metric, group_col)
  overall <- data.frame(
    metric  = metric,
    method  = st$method,
    p_global = st$global_p
  )
  utils::write.csv(
    overall,
    file      = file.path(outdir, paste0("alpha_overall_", metric, ".csv")),
    row.names = FALSE
  )
  
  if (!is.null(st$posthoc) && nrow(st$posthoc) > 0) {
    utils::write.csv(
      st$posthoc,
      file      = file.path(outdir, paste0("alpha_posthoc_", metric, ".csv")),
      row.names = FALSE
    )
  }
  if (!is.null(st$shapiro)) {
    utils::write.csv(
      st$shapiro,
      file      = file.path(outdir, paste0("alpha_", metric, "_shapiro_by_group.csv")),
      row.names = FALSE
    )
  }
  if (!is.null(st$levene)) {
    utils::write.csv(
      st$levene,
      file      = file.path(outdir, paste0("alpha_", metric, "_levene.csv")),
      row.names = FALSE
    )
  }
  invisible(st)
}

invisible(lapply(
  c("Chao1", "Shannon", "Simpson", "Pielou"),
  function(m) save_alpha_diagnostics(alpha_df, m, group_col, outdir = EXPORT_DIR)
))

# Save alpha plots with explicit sizes
save_plot_multi(
  p_alpha1, file.path(EXPORT_DIR, "alpha_chao1"),
  png_width  = 5, png_height  = 6, png_units  = "in",
  svg_width  = 12, svg_height = 14, svg_units = "cm"
)
save_plot_multi(
  p_alpha2, file.path(EXPORT_DIR, "alpha_shannon"),
  png_width  = 5, png_height  = 6, png_units  = "in",
  svg_width  = 12, svg_height = 14, svg_units = "cm"
)
save_plot_multi(
  p_alpha3, file.path(EXPORT_DIR, "alpha_simpson"),
  png_width  = 5, png_height  = 6, png_units  = "in",
  svg_width  = 12, svg_height = 14, svg_units = "cm"
)
save_plot_multi(
  p_alpha4, file.path(EXPORT_DIR, "alpha_pielou"),
  png_width  = 5, png_height  = 6, png_units  = "in",
  svg_width  = 12, svg_height = 14, svg_units = "cm"
)

# ============================================================
# 7) Beta diversity (distances, PCoA, NMDS, PERMANOVA, betadisper)
# ============================================================

# Minimum library size for beta (remove samples with fewer reads)
lib_beta   <- rowSums(counts_mat, na.rm = TRUE)
keep_beta  <- lib_beta >= min_lib_beta
counts_mat_beta <- counts_mat[keep_beta, , drop = FALSE]
meta_beta        <- meta[
  meta[[sample_id_col]] %in% rownames(counts_mat_beta),
  , drop = FALSE
]
stopifnot(all(rownames(counts_mat_beta) == meta_beta[[sample_id_col]]))

if (!group_col %in% names(meta_beta)) {
  meta_beta[[group_col]] <- "All"
  message("`", group_col, "` not found in metadata; using a single group 'All' for beta.")
}

# Distance methods to compute
beta_methods <- c("bray", "jaccard", "aitchison")

# Explicit plot sizes per method / plot
beta_sizes <- list(
  bray = list(
    pcoa_png = c(18, 12, "cm"), pcoa_svg = c(18, 12, "cm"),
    nmds_png = c(18, 12, "cm"), nmds_svg = c(18, 12, "cm")
  ),
  jaccard = list(
    pcoa_png = c(18, 12, "cm"), pcoa_svg = c(18, 12, "cm"),
    nmds_png = c(18, 12, "cm"), nmds_svg = c(18, 12, "cm")
  ),
  aitchison = list(
    pcoa_png = c(18, 12, "cm"), pcoa_svg = c(18, 12, "cm"),
    nmds_png = c(18, 12, "cm"), nmds_svg = c(18, 12, "cm")
  )
)

# Simple write-permission test for the output directory
cat("write-test\n", file = file.path(EXPORT_DIR, ".__write_test__.txt"))

# Diagnostic saver for beta plots (prints paths and checks files)
save_plot_checked <- function(plot, file_png, file_svg,
                              png_width, png_height,
                              png_units = "in", png_dpi = 300,
                              svg_width, svg_height,
                              svg_units = "cm") {
  dir.create(dirname(file_png), showWarnings = FALSE, recursive = TRUE)
  
  message("[Saving PNG] ", normalizePath(file_png, mustWork = FALSE))
  ok_png <- tryCatch({
    ggplot2::ggsave(
      filename = file_png,
      plot     = plot,
      width    = as.numeric(png_width),
      height   = as.numeric(png_height),
      units    = png_units,
      dpi      = png_dpi
    )
    TRUE
  }, error = function(e) {
    message("PNG save error: ", e$message)
    FALSE
  })
  if (ok_png && !file.exists(file_png)) {
    message("Warning: PNG not found after save: ", file_png)
  }
  
  message("[Saving SVG] ", normalizePath(file_svg, mustWork = FALSE))
  ok_svg <- tryCatch({
    ggplot2::ggsave(
      filename = file_svg,
      plot     = plot,
      device   = svglite::svglite,
      width    = as.numeric(svg_width),
      height   = as.numeric(svg_height),
      units    = svg_units
    )
    TRUE
  }, error = function(e) {
    message("SVG save error: ", e$message)
    FALSE
  })
  if (ok_svg && !file.exists(file_svg)) {
    message("Warning: SVG not found after save: ", file_svg)
  }
  
  invisible(ok_png && ok_svg)
}

# Core beta-diversity pipeline for a given distance metric
run_beta_pipeline <- function(metric = c("bray", "jaccard", "aitchison"),
                              show_plots = TRUE) {
  metric <- match.arg(metric)
  
  Xraw <- counts_mat_beta
  grp  <- factor(meta_beta[[group_col]])
  n_groups <- nlevels(grp)
  if (nrow(Xraw) < 2) {
    stop("Beta: need at least two samples after filtering.")
  }
  
  # Distance matrix
  if (metric == "bray") {
    X <- Xraw / pmax(rowSums(Xraw), 1)
    dist_obj <- vegan::vegdist(X, method = "bray")
  } else if (metric == "jaccard") {
    X <- (Xraw > 0) * 1
    dist_obj <- vegan::vegdist(X, method = "jaccard", binary = TRUE)
  } else if (metric == "aitchison") {
    X <- .clr(Xraw)
    dist_obj <- stats::dist(X, method = "euclidean")
  }
  
  if (anyNA(as.matrix(dist_obj))) {
    stop("Beta: distance matrix contains NA. Check for empty samples / IDs.")
  }
  
  # PCoA (cmdscale) with fallback to ape::pcoa (Cailliez correction)
  pc <- tryCatch(
    cmdscale(dist_obj, eig = TRUE, k = 2),
    error = function(e) NULL
  )
  if (is.null(pc) || anyNA(pc$points)) {
    pc2   <- ape::pcoa(dist_obj, correction = "cailliez")
    pvar  <- round(100 * pc2$values$Relative_eig[1:2], 1)
    pcoa_df <- data.frame(
      SampleID = rownames(Xraw),
      Axis1    = pc2$vectors[, 1],
      Axis2    = pc2$vectors[, 2]
    ) %>%
      dplyr::left_join(meta_beta, by = "SampleID")
  } else {
    pvar <- percent_from_eig(pc$eig)
    pcoa_df <- data.frame(
      SampleID = rownames(Xraw),
      Axis1    = pc$points[, 1],
      Axis2    = pc$points[, 2]
    ) %>%
      dplyr::left_join(meta_beta, by = "SampleID")
  }
  
  # NMDS
  nmds <- tryCatch(
    suppressWarnings(
      vegan::metaMDS(
        dist_obj,
        k            = 2,
        trymax       = 100,
        autotransform = FALSE,
        trace        = FALSE
      )
    ),
    error = function(e) {
      message("NMDS(", metric, ") failed: ", e$message)
      NULL
    }
  )
  nmds_df <- if (!is.null(nmds)) {
    data.frame(
      SampleID = rownames(Xraw),
      NMDS1    = nmds$points[, 1],
      NMDS2    = nmds$points[, 2]
    ) %>%
      dplyr::left_join(meta_beta, by = "SampleID")
  } else {
    NULL
  }
  
  # PERMANOVA and betadisper
  bd      <- NULL
  bd_perm <- NULL
  ad_glob <- NULL
  pw      <- NULL
  if (n_groups >= 2) {
    bd      <- vegan::betadisper(dist_obj, group = grp)
    bd_perm <- vegan::permutest(bd, permutations = 999)
    ad_glob <- vegan::adonis2(dist_obj ~ grp, permutations = 999)
    pw      <- pairwise_permanova(dist_obj, grp, permutations = 999)
    sub_perma <- paste0(
      "PERMANOVA: F=", round(ad_glob$F[1], 3),
      " | R²=", round(ad_glob$R2[1], 3),
      " | p=", formatC(ad_glob$`Pr(>F)`[1], format = "g", digits = 3)
    )
  } else {
    sub_perma <- "PERMANOVA: n/a (< 2 groups)"
  }
  
  # Ordination plots
  can_ellipse <- all(table(grp) >= 3)
  
  p_pcoa <- ggplot(
    pcoa_df,
    aes(x = Axis1, y = Axis2, color = .data[[group_col]])
  ) +
    geom_point(size = 3, alpha = 0.9, na.rm = TRUE) +
    ggrepel::geom_text_repel(
      aes(label = SampleID),
      size         = 3,
      max.overlaps = 20,
      show.legend  = FALSE,
      na.rm        = TRUE
    ) +
    labs(
      x      = paste0("PCoA1 (", pvar[1], "%)"),
      y      = paste0("PCoA2 (", pvar[2], "%)"),
      color  = group_col,
      title  = paste0("PCoA - ", tools::toTitleCase(metric)),
      subtitle = sub_perma
    ) +
    theme_bw()
  
  if (can_ellipse) {
    p_pcoa <- p_pcoa +
      ggplot2::stat_ellipse(
        aes(group = .data[[group_col]]),
        linewidth   = 0.6,
        alpha       = 0.6,
        show.legend = FALSE
      )
  }
  
  if (!is.null(nmds_df)) {
    p_nmds <- ggplot(
      nmds_df,
      aes(x = NMDS1, y = NMDS2, color = .data[[group_col]])
    ) +
      geom_point(size = 3, alpha = 0.9, na.rm = TRUE) +
      ggrepel::geom_text_repel(
        aes(label = SampleID),
        size         = 3,
        max.overlaps = 20,
        show.legend  = FALSE,
        na.rm        = TRUE
      ) +
      labs(
        x      = "NMDS1",
        y      = "NMDS2",
        color  = group_col,
        title  = paste0("NMDS - ", tools::toTitleCase(metric)),
        subtitle = paste0("stress = ", round(nmds$stress, 3),
                          " | ", sub_perma)
      ) +
      theme_bw()
    
    if (can_ellipse) {
      p_nmds <- p_nmds +
        ggplot2::stat_ellipse(
          aes(group = .data[[group_col]]),
          linewidth   = 0.6,
          alpha       = 0.6,
          show.legend = FALSE
        )
    }
  } else {
    p_nmds <- NULL
  }
  
  # Preview plots (optional)
  if (isTRUE(show_plots)) {
    message("[Preview] PCoA - ", metric)
    print(p_pcoa)
    if (!is.null(p_nmds)) {
      message("[Preview] NMDS - ", metric)
      print(p_nmds)
    }
  }
  
  # Export distance matrix and ordination scores
  utils::write.csv(
    as.matrix(dist_obj),
    file      = file.path(EXPORT_DIR, paste0("distance_", metric, ".csv")),
    row.names = TRUE
  )
  data.table::fwrite(
    pcoa_df,
    file = file.path(EXPORT_DIR, paste0("pcoa_scores_", metric, ".csv"))
  )
  if (!is.null(nmds_df)) {
    data.table::fwrite(
      nmds_df,
      file = file.path(EXPORT_DIR, paste0("nmds_scores_", metric, ".csv"))
    )
  }
  
  # betadisper outputs (robust to version differences)
  if (!is.null(bd_perm) && !is.null(bd_perm$tab)) {
    bd_perm_tab <- as.data.frame(bd_perm$tab)
    nperm <- tryCatch({
      if (!is.null(bd_perm$permutations)) {
        if (is.numeric(bd_perm$permutations)) {
          as.integer(bd_perm$permutations)
        } else {
          as.integer(attr(bd_perm$permutations, "nperm"))
        }
      } else if (!is.null(attr(bd_perm, "permutations"))) {
        as.integer(attr(bd_perm, "permutations"))
      } else if (!is.null(bd_perm$perm)) {
        np <- attr(bd_perm$perm, "nperm")
        if (is.null(np)) as.integer(length(bd_perm$perm)) else as.integer(np)
      } else {
        NA_integer_
      }
    }, error = function(e) NA_integer_)
    
    if (length(nperm) == 1 && is.finite(nperm)) {
      bd_perm_tab$N.Perm <- rep(nperm, nrow(bd_perm_tab))
    }
    
    utils::write.csv(
      bd_perm_tab,
      file      = file.path(EXPORT_DIR, paste0("betadisper_global_", metric, ".csv")),
      row.names = TRUE
    )
    
    tuk <- tryCatch(TukeyHSD(bd), error = function(e) NULL)
    if (!is.null(tuk)) {
      for (nm in names(tuk)) {
        tk <- as.data.frame(tuk[[nm]])
        tk$contrast <- rownames(tk)
        utils::write.csv(
          tk,
          file      = file.path(EXPORT_DIR, paste0("betadisper_tukey_", metric, "_", nm, ".csv")),
          row.names = FALSE
        )
      }
    }
  } else if (!is.null(bd_perm)) {
    message("betadisper: permutest returned no table; skipping CSV export.")
  }
  
  ad_glob_df <- tryCatch({
    df <- as.data.frame(ad_glob)
    df$Term <- rownames(df)
    rownames(df) <- NULL
    df
  }, error = function(e) {
    data.frame(Term = "grp", Note = "PERMANOVA skipped (< 2 groups)")
  })
  
  utils::write.csv(
    ad_glob_df,
    file      = file.path(EXPORT_DIR, paste0("permanova_global_", metric, ".csv")),
    row.names = FALSE
  )
  if (!is.null(pw) && nrow(pw) > 0) {
    utils::write.csv(
      pw,
      file      = file.path(EXPORT_DIR, paste0("permanova_pairwise_", metric, ".csv")),
      row.names = FALSE
    )
  }
  
  list(
    metric   = metric,
    p_pcoa   = p_pcoa,
    p_nmds   = p_nmds,
    pcoa_df  = pcoa_df,
    nmds_df  = nmds_df
  )
}

# Run all requested beta methods
beta_results <- lapply(
  beta_methods,
  function(m) {
    tryCatch(
      run_beta_pipeline(m, show_plots = TRUE),
      error = function(e) {
        message("Method ", m, " failed: ", e$message)
        NULL
      }
    )
  }
)
names(beta_results) <- beta_methods
beta_results <- Filter(Negate(is.null), beta_results)

message("Beta-diversity results available for: ",
        paste(names(beta_results), collapse = ", "))

# Save beta plots for each metric
for (m in names(beta_results)) {
  r <- beta_results[[m]]
  s <- beta_sizes[[m]]
  if (is.null(s)) {
    message("No plot sizes declared for method: ", m)
    next
  }
  
  # PCoA
  pcoa_png_w <- as.numeric(s$pcoa_png[[1]])
  pcoa_png_h <- as.numeric(s$pcoa_png[[2]])
  pcoa_png_u <- as.character(s$pcoa_png[[3]])
  pcoa_svg_w <- as.numeric(s$pcoa_svg[[1]])
  pcoa_svg_h <- as.numeric(s$pcoa_svg[[2]])
  pcoa_svg_u <- as.character(s$pcoa_svg[[3]])
  
  save_plot_checked(
    r$p_pcoa,
    file_png  = file.path(EXPORT_DIR, paste0("pcoa_", m, ".png")),
    file_svg  = file.path(EXPORT_DIR, paste0("pcoa_", m, ".svg")),
    png_width = pcoa_png_w,
    png_height = pcoa_png_h,
    png_units  = pcoa_png_u,
    svg_width  = pcoa_svg_w,
    svg_height = pcoa_svg_h,
    svg_units  = pcoa_svg_u
  )
  
  # NMDS
  if (!is.null(r$p_nmds)) {
    nmds_png_w <- as.numeric(s$nmds_png[[1]])
    nmds_png_h <- as.numeric(s$nmds_png[[2]])
    nmds_png_u <- as.character(s$nmds_png[[3]])
    nmds_svg_w <- as.numeric(s$nmds_svg[[1]])
    nmds_svg_h <- as.numeric(s$nmds_svg[[2]])
    nmds_svg_u <- as.character(s$nmds_svg[[3]])
    
    save_plot_checked(
      r$p_nmds,
      file_png  = file.path(EXPORT_DIR, paste0("nmds_", m, ".png")),
      file_svg  = file.path(EXPORT_DIR, paste0("nmds_", m, ".svg")),
      png_width = nmds_png_w,
      png_height = nmds_png_h,
      png_units  = nmds_png_u,
      svg_width  = nmds_svg_w,
      svg_height = nmds_svg_h,
      svg_units  = nmds_svg_u
    )
  }
}

# List key output files for convenience
message("Beta analysis done. Outputs in: ",
        normalizePath(EXPORT_DIR, mustWork = FALSE))
print(list.files(
  EXPORT_DIR,
  pattern = "pcoa|nmds|distance|permanova|betadisper",
  full.names = TRUE
))
